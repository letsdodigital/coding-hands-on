<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>curriculum_imaging</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="curriculum_imaging_files/libs/clipboard/clipboard.min.js"></script>
<script src="curriculum_imaging_files/libs/quarto-html/quarto.js"></script>
<script src="curriculum_imaging_files/libs/quarto-html/popper.min.js"></script>
<script src="curriculum_imaging_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="curriculum_imaging_files/libs/quarto-html/anchor.min.js"></script>
<link href="curriculum_imaging_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="curriculum_imaging_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="curriculum_imaging_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="curriculum_imaging_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="curriculum_imaging_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="daim-imaging-learning-outcomes" class="level1">
<h1>DAIM (Imaging) Learning Outcomes</h1>
<p>This document details the high level learning outcomes for each seminar and workshop in the course.</p>
<p>The following learning levels (LLs) should be able to be mapped to the learning points.</p>
<p>1. <strong>Level 1 - Basic Practical Skills (coding skills)</strong> * E.g. How to reshape a NumPy array from <code>(3, 3, 3)</code> into <code>(9, 3)</code>.</p>
<p>2. <strong>Level 2 - Theory (modeling skills)</strong> * E.g. How k-means splits data up into clusters</p>
<p>3. <strong>Level 3 - Project Planning and Management</strong> * E.g. How to write a class for plotting similar plots when opening a DICOM file.</p>
<p>4. <strong>Level 4 - Clinical Translation</strong> * E.g. What format data needs to be collected in to train a clinical outcome predictor from chest x-ray data.</p>
<p>The seminars will consist mainly of LL2 and LL4 information, whereas workshops will consist of LL1 and LL3 information. The workshops at the start of the course should feature extension tasks which are more complex for people to work through in the workshops if they finish the content before the workshop happens.</p>
</section>
<section id="overall-structure" class="level1">
<h1>Overall Structure</h1>
<ul>
<li>The course will be split into the following modules:
<ul>
<li>Module 0 - Overview (no workshop)</li>
<li>Module 1 - Introduction (no workshop)</li>
<li>Module 2 - Core Python for Image Processing
<ul>
<li>This can be stretched over multiple weeks depending on the skills and level of the people on the course</li>
</ul></li>
<li>Module 3 - Python and DICOM
<ul>
<li>This will include the coding skills needed to open and use DICOM files</li>
<li>The workshop will include a mixture of work to do outside a session and tasks to do within a session with a tutor.</li>
</ul></li>
<li>Module 4 - AI for Medicine
<ul>
<li>This will be a set of two or more seminars with a long workbook that will be worked through over multiple weeks</li>
<li>The work in the workbook will be mostly independent.</li>
</ul></li>
<li>Module 5 - Closing (no workshop)</li>
</ul></li>
</ul>
</section>
<section id="module-1---course-introduction" class="level1">
<h1>Module 1 - Course Introduction</h1>
<section id="seminar" class="level2">
<h2 class="anchored" data-anchor-id="seminar">Seminar</h2>
<p>By the end of the seminar, clinicians should be able to:</p>
<ul>
<li>Know the overall aims and philosophy of the course</li>
<li>Aims of the DAIM courses
<ul>
<li>Translate a basic knowledge of Python into workable clinical skill</li>
<li>Inspire the individual to seek further resources to learn more about the topic</li>
<li>Empower the learner to build complex applications</li>
</ul></li>
<li>General coding information
<ul>
<li>Know the layout of a Jupyter Notebook and how to use it (LL1)</li>
<li>Know the layout of documentation and how to use it (LL3)</li>
<li>Know what StackOverflow is and when to use it (LL3)</li>
<li>Understand what constitutes “machine readable” data and what considerations clinicians must make to maximise this (LL4)</li>
</ul></li>
</ul>
</section>
</section>
<section id="module-2---core-python-for-image-processing" class="level1">
<h1>Module 2 - Core Python for Image Processing</h1>
<section id="seminar-1" class="level2">
<h2 class="anchored" data-anchor-id="seminar-1">Seminar</h2>
<p>By the end of the seminar, clinicians should:</p>
<ul>
<li>Know general uses of image data in clinical settings</li>
<li>Digital image representation
<ul>
<li>Understand what a pixel is, and the values it can take on (0.0-1.0, 0-255) (LL2)</li>
<li>Understand the difference between greyscale and colour images (LL2)</li>
<li>Understand how brightness maps on value (LL2)</li>
<li></li>
<li></li>
<li>Understand different ways to represent colours (RGB, HSV) (LL2)</li>
<li>Be able to read a hex colour code (LL2)</li>
<li>Understand what the dimensions of an image represent (LL2)</li>
<li>Understand what the difference between compressed and uncompressed images are (LL2)</li>
<li>Understand the difference between lossy and lossless compression (LL2)</li>
<li>Be able to list different image formats (LL2)</li>
<li>Understand how these concepts extend to 3D images (volumetric images) (LL2)</li>
</ul></li>
<li>Convolution
<ul>
<li>Understand why convolution has been chosen as the example transform for this course (LL2)
<ul>
<li>Use in ML, easy to visualise</li>
</ul></li>
<li>Understand what convolution is and what it is used for (LL2)</li>
<li>Understand what a convolution kernel is (LL2)</li>
<li>Understand what types of operation can be done with convolution (blurring, sharpening) (LL2)</li>
<li>Understand intuitively how to average a signal using an averaging kernel e.g.&nbsp;<code>[0.2, 0.2, 0.2, 0.2, 0.2]</code> (LL2)</li>
<li>Appreciate that convolution can be used in less than or more than 2 dimensions (LL2)</li>
</ul></li>
<li>Basics of PIL (LL1)
<ul>
<li>Why PIL is valuable in the Python ecosystem (LL3)</li>
<li>The sorts of operations that PIL can do (LL1)</li>
</ul></li>
<li>Basics of NumPy (LL1)
<ul>
<li>Know why NumPy exists within the Python ecosystem (speed of array operations, ease of working with multidimensional data) (LL2)</li>
<li>Understand the concept of multidimensional arrays and the ‘shape’ of an array (LL2)</li>
<li>Explain what a NumPy array is (LL2)</li>
<li>Understand how to use NumPy functions and common pitfalls e.g.&nbsp;shape mismatches (LL1)</li>
</ul></li>
<li>Explain how to represent common data structures as NumPy data (quiz-style questions to test understanding) (LL2)
<ul>
<li>Image data - NumPy array</li>
</ul></li>
</ul>
</section>
<section id="workshop" class="level2">
<h2 class="anchored" data-anchor-id="workshop">Workshop</h2>
<p>The aim of this workshop is to recap some basic Python skills and to ensure that course attendees have the appropriate knowledge of core packages before starting more in-depth content. The NumPy section will focus on combining and modifying arrays and using basic library functions. An task using convolution will be used to demonstrate this to conceptually prepare for the machine learning section of the course.</p>
<p>Use an example X-ray image with a simulated CSV file describing an imaginary small dataset of patients.</p>
<p>By the end of the workshop, clinicians should be able to:</p>
<ul>
<li>Basics of PIL
<ul>
<li>Open an image from disk with PIL (LL1)</li>
<li>Show an image with PIL (LL1)</li>
<li>Rotate an image with PIL (LL1)</li>
<li>Crop an image with PIL (LL1)</li>
<li>Convert to greyscale with PIL (LL1)</li>
<li>Understand the internal representation of images within PIL (LL1)</li>
</ul></li>
<li>NumPy
<ul>
<li>Open a CSV file into a NumPy array (LL1)</li>
<li>Convert a column of a DataFrame to a NumPy array (LL1)</li>
<li>Print the shape of a NumPy array (LL1)</li>
<li>Accessing array elements using array slicing (e.g.&nbsp;<code>[0:4]</code>, <code>[:-1]</code>)</li>
<li>Count the number of unique items in an array with <code>numpy.unique()</code> (LL1)</li>
</ul></li>
<li>Convolution example task (LL1)
<ul>
<li>Create a new array (convolution kernel) of ones of a specified shape (LL1)</li>
<li>Divide this new array by its length (LL1)</li>
<li>Convolve the kernel with the NumPy array using <code>np.convolve()</code> (LL1)</li>
<li>Display this image with PIL (LL1)</li>
<li>Smooth an image with PIL and a custom convolution kernel (LL1)</li>
<li>Sharpen an image with PIL and a custom convolution kernel (LL1)</li>
<li>Combine all of these into a function which takes an argument to specify a type of operation to perform (LL1)</li>
</ul></li>
</ul>
</section>
</section>
<section id="module-3---python-and-dicom" class="level1">
<h1>Module 3 - Python and DICOM</h1>
<p>The aim of this module is to give clinicians an introduction into the DICOM file format, what type of data is represented in this file format, and how to extract basic 2D and 3D image data from these files.</p>
<p><em><strong>N.B.</strong> The interpretation of images in any form will not be covered in this unit.</em></p>
<section id="seminar-2" class="level2">
<h2 class="anchored" data-anchor-id="seminar-2">Seminar</h2>
<p>By the end of the seminar, clinicians should:</p>
<ul>
<li>Understand the aims of this module (LL4)</li>
<li>Basics of image representation (LL2)
<ul>
<li>Understand what a pixel is (LL2)</li>
<li>Understand what a voxel is, by extension (LL2)</li>
<li>Greyscale images
<ul>
<li>Understand what values greyscale values are are usually given in (0.0-1.0, 0-255, HU) (LL2)</li>
<li>Understand what a Houndsfield Unit is (LL2)</li>
</ul></li>
<li>Colours
<ul>
<li>Understand how RGB data is used to represent most images (LL2)</li>
<li>Understand what a colourmap is and when it is used (LL2)</li>
</ul></li>
</ul></li>
<li>The DICOM format
<ul>
<li>Know the history of the DICOM standard and why it was introduced (LL4)</li>
<li>Understand what information is contained within a DICOM file (LL2)</li>
<li>Understand that DICOM is a lossless standard (LL2)</li>
<li>Understand broadly how a DICOM file stores image data (both 2D and 3D) (LL2)</li>
<li>Understand why Pydicom is a useful tool for clinicians (LL4)</li>
</ul></li>
<li>Understand what considerations are needed when coding in Python to convert images, with interactive questions (array shapes) (LL3)</li>
<li>Understand the difference between volumetric 3D images and 3D meshes and what is needed to convert between them (segmentation)</li>
<li>Understand the basics of image segmentation in 2D and 3D (LL2)</li>
<li>Understand where volumetric imaging is used and where mesh reconstructions are used in clinical medicine (LL4)</li>
</ul>
</section>
<section id="workshop-1" class="level2">
<h2 class="anchored" data-anchor-id="workshop-1">Workshop</h2>
<p>By the end of the workshop, clinicians should be:</p>
<ul>
<li>Open a DICOM file with Python (LL1)</li>
<li>Scan information
<ul>
<li>Understand that there are three different types of element in a DICOM file (LL2)</li>
<li>Understand different ways to access elements (by hex tag and keyword) (LL2)</li>
<li>Extract basic scan information by keyword e.g.&nbsp;<code>PatientName</code> (LL1)</li>
</ul></li>
<li>Image extraction
<ul>
<li>Understand what data is needed to accurately reconstruct data from a DICOM study for 3D and 2D images (image shape, slice indices) (LL1)</li>
<li>Extract 2D image data into a NumPy array using <code>ds.pixel_array</code> (LL1)</li>
<li>Extract 2D colourmap data (<em>e.g.&nbsp;Doppler flow rate data</em>) and plot this appropriately (LL1)</li>
<li>Reconstruct 3D image from a DICOM study and <a href="https://pydicom.github.io/pydicom/stable/auto_examples/image_processing/reslice.html#sphx-glr-auto-examples-image-processing-reslice-py">plot axial, sagittal and coronal views</a> (LL1)</li>
<li>Preprocess the image for display in Python (LL1)</li>
<li>Plot image data from a DICOM file in Python (LL1)</li>
</ul></li>
</ul>
<p>We need a candidate DICOM study/studies for this portion of the course. Options include: - <a href="https://www.pcir.org/">Patient Contributed Image Repository</a> - This is where Pydicom gets it’s official example studies. - Pydicom’s official test files</p>
<p>We need the following studies: - 2D basic scan (e.g.&nbsp;CXR) - 3D volumetric scan (e.g.&nbsp;CT head)</p>
<p><em>We could also add an extra section to the end to work with colourmaps. This would need imaging with colour data - e.g.&nbsp;ultrasound with flow data.</em></p>
</section>
</section>
<section id="module-4---ai-for-medicine" class="level1">
<h1>Module 4 - AI for Medicine</h1>
<section id="seminar-1-1" class="level2">
<h2 class="anchored" data-anchor-id="seminar-1-1">Seminar 1</h2>
<p>By the end of the first seminar, clinicians should:</p>
<ul>
<li>Understand the aims of this first AI module and what it will cover</li>
<li>Understand what machine learning is at a broad level (LL2)</li>
<li>Be able to list clinical uses for ML (LL4)</li>
<li>Be able to describe the high-level steps needed to train a statistical model (collect data, preprocess data, augment data, define model, compile model, train model, evaluate model) (LL2)</li>
<li>Datasets
<ul>
<li>Understanding the importance of splitting the data into training, testing, and validation datasets (LL2)</li>
<li>Understand dataset bias and clincial factors that can confound a dataset (LL4)</li>
<li>Use examples to demonstrate clinical bias <em>TODO find appropriate examples</em> (LL4)</li>
</ul></li>
<li>Be able to relate the aims of the example task (Pneumonia detection task) (LL4)
<ul>
<li>Disclaimer that this course will not cover intepretation of chest X-rays</li>
</ul></li>
<li>Data preprocessing
<ul>
<li>Understand considerations that must be made when standardising image data for model input (image size, image dimensions, contrast, brightness) (LL2)</li>
<li>Understand the importance of data augmentation when training (LL2)</li>
<li>Be able to name common data augmentation techniques (shearing, rotation, stretching, elastic deformation in medical imaging datasets) (LL2)</li>
<li>Understand how these concepts apply to other types of data (e.g.&nbsp;time-series data) (LL2)</li>
</ul></li>
<li>Classifier evaluation
<ul>
<li>Evaulating models</li>
<li>Describe methods for evaulating regression models (LL2)</li>
<li>Be able to describe important metrics for a classifier (positive/negative predictive value, sensitivity and specificity) (LL2)</li>
<li>Be able to describe the statistical tests needed to evaluate a binary classifier (F1 score, AUC and ROC curves) (LL2)</li>
<li>Be able to give a broad overview of the common metrics that are used with other types of model (softmax for multiclass classifiers, pixel accuracy and intersection over union (IoU) for segmentation) (LL2)</li>
</ul></li>
</ul>
</section>
<section id="seminar-2-1" class="level2">
<h2 class="anchored" data-anchor-id="seminar-2-1">Seminar 2</h2>
<p>By the end of the seminar, clinicians should:</p>
<ul>
<li>Understand the difference between supervised and unsupervised learning (LL2)</li>
<li>Understand what a loss function is (LL2)
<ul>
<li>Appreciate that there are a wide variety of optimisers and loss functions that can be used (LL2)</li>
</ul></li>
<li>Gain an intuition for how training via backpropagation works via the ball on a hill analogy (LL2)
<ul>
<li>Link to 3Blue1Brown videos</li>
</ul></li>
<li>Gain an intuitive understanding of overfitting vs underfitting using a linear model as an example (LL2)</li>
<li>Model architecture and training
<ul>
<li>Understand why ML frameworks exist in the Python ecosystem (TensorFlow, PyTorch) and what they allow the user to do (LL2)</li>
<li>Understand the inputs and outputs of a neural network (LL2)</li>
<li>Understand the difference between a convolutional layer and dense layer at a broad level (LL2)</li>
<li>Understand what an epoch of training is (LL2)</li>
<li>Understand why data needs to be batched when training a network (LL3)</li>
<li>Understand what a sigmoid layer is and why it is used for probabilistic outputs (LL2)</li>
</ul></li>
<li>Hyperparameters
<ul>
<li>Understand what a hyperparameter is (LL2)</li>
<li>How a hyper-parameter effects model output (LL2)</li>
<li>Understand that hyperparameters can be tuned (LL2)</li>
</ul></li>
<li>Common pitfalls
<ul>
<li>How to deal with overfitting. (LL2)</li>
<li>How to deal with multicollinearity (LL2)</li>
<li>How to deal with missing data and sensitivity analysis (LL2)</li>
</ul></li>
<li>Appreciate that the task we are performing is fairly simple and that more complex techniques exist (LL2)</li>
<li>Be able to discuss the uses, advantages, and disadvantages of more modern approaches to machine learning (stable diffusion, large language models) (LL2) (LL4)</li>
</ul>
<p><em>Maybe a radiologist/senior clinician could speak about the features that the bot may be dectecting in each scan?</em></p>
</section>
<section id="workshop-2" class="level2">
<h2 class="anchored" data-anchor-id="workshop-2">Workshop</h2>
<p>The pneumonia dataset that will be used is <a href="https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data">this one</a>. It has a CC BY 4.0 licence, which means it is free to use for commercial use. Dataset features include: - 2GB size - course participants will download the dataset directly to the Colab instance and decompress it using the following commands:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!kaggle</span> datasets download <span class="at">-d</span> paultimothymooney/chest-xray-pneumonia</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">!unzip</span> chest-xray-pneumonia.zip</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This workshop should be run as one over two seminars. The seminar can be used to check in on how people are doing with the workshop content as this will be unsupported. There will be a break point in the workshop (trained model saved) which is used between seminars.</p>
<p>By the end of the workshop, clinicians should be able to:</p>
<ul>
<li>Data preprocessing
<ul>
<li>Open an image using PIL with <code>Image.open()</code> and extract basic information about the image (format, size) (LL1)</li>
<li>Centre crop an image to standardise for machine learning (LL1)</li>
<li>Resize images using PIL method <code>Image.resize()</code> (LL1)</li>
<li>Convert an image to grayscale (LL1)</li>
<li>Pack greyscale images into a 3 dimensional array for training and testing (LL1)</li>
<li>Understand why using Python generators is important for loading data (LL3)</li>
<li>Construct a generator using the <code>yield</code> command (LL1)</li>
</ul></li>
<li>Binary classifier model evaulation
<ul>
<li>Evaluate the performance of a simulated diagnostic classifier using familiar statistics from evidence-based medicine training (LL1)</li>
<li>Calculate and understand precision, recall, and F1 scores (LL1) (LL2)</li>
</ul></li>
</ul>
<p><strong>Break between seminars.</strong></p>
<ul>
<li>Model architecture
<ul>
<li>Understand the difference between <code>Sequential</code> models and other models in Keras (LL2)</li>
<li>Understand how to construct a basic convolutional neural network using Keras/TensorFlow (LL1)</li>
<li>Understand how to compile a model and view its architecture using <code>model.summary()</code> (LL1)</li>
</ul></li>
<li>Model training
<ul>
<li>Understand how to run a training loop for a neural network (LL1)</li>
</ul></li>
<li>Model saving
<ul>
<li>Structure machine learning projects to to allow for reuse of model weights (LL3)</li>
<li>Save files from Google Colab for reuse later (LL1)</li>
<li>Open a machine learning model from file (LL1)</li>
</ul></li>
<li>Model tuning
<ul>
<li>Appropriately tune hyperparameters and model architecture to improve model performance and size (LL1)</li>
<li><em>Note: This should be done in conjunction with the model evaulation step below.</em></li>
</ul></li>
<li>Binary classifier model evaulation
<ul>
<li>Evaulate a model using testing data (LL1)</li>
<li>Calculate and understand precision, recall, and F1 scores (LL1)</li>
<li>Use pyplot and scipy to plot a formal ROC curve for a binary classifier (LL1)</li>
<li>Calculate the AUC value for this curve (LL1)</li>
</ul></li>
</ul>
<p>The model structure will be a convolutional neural network with single sigmoid output node denoting probability of pneumonia in the scan.</p>
</section>
</section>
<section id="session-5---closing-session" class="level1">
<h1>Session 5 - Closing Session</h1>
<section id="seminar-3" class="level2">
<h2 class="anchored" data-anchor-id="seminar-3">Seminar</h2>
<p>By the end of the seminar, clinicians should: - Understand the core learning outcomes from the course - Understand how to grow their skills further by using other available resources (LL4) - Give feedback for further improvements to the course (LL4)</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>